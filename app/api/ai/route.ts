import { NextRequest, NextResponse } from "next/server";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { buildPortfolioContext } from "@/lib/ai/portfolioContext";
import { getSystemPrompt } from "@/lib/ai/persona";
import { checkRateLimit } from "@/lib/ai/rateLimiter";

// Server-side only - API key never exposed to client
const apiKey = process.env.GEMINI_API_KEY || "";
const genAI = apiKey ? new GoogleGenerativeAI(apiKey) : null;

const MAX_INPUT_LENGTH = 1500;
const COOLDOWN_MS = 3000;

const lastRequestTime = new Map<string, number>();

export async function POST(request: NextRequest) {
  try {
    // Check API key configuration
    if (!genAI || !apiKey) {
      return NextResponse.json(
        { error: "AI is not configured." },
        { status: 500 }
      );
    }

    // Parse request
    const body = await request.json();
    const { message, lang = "en", mode = "recruiter" } = body;

    // Validate input
    if (!message || typeof message !== "string") {
      return NextResponse.json(
        { error: "Invalid request" },
        { status: 400 }
      );
    }

    if (message.length > MAX_INPUT_LENGTH) {
      return NextResponse.json(
        {
          error:
            lang === "ko"
              ? "ì§ˆë¬¸ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (ìµœëŒ€ 1,500ì)"
              : "Question too long (max 1,500 characters)",
        },
        { status: 400 }
      );
    }

    // Get client IP for rate limiting
    const ip = request.headers.get("x-forwarded-for") || "unknown";
    
    // Check rate limit (20 requests/hour)
    const rateLimit = checkRateLimit(ip, 20, 60 * 60 * 1000);
    if (!rateLimit.allowed) {
      return NextResponse.json(
        {
          error:
            lang === "ko"
              ? "ìš”ì²­ í•œë„ ì´ˆê³¼. 1ì‹œê°„ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
              : "Rate limit exceeded. Please try again in 1 hour.",
        },
        { status: 429 }
      );
    }

    // Cooldown check (prevent spam)
    const lastTime = lastRequestTime.get(ip) || 0;
    const now = Date.now();
    if (now - lastTime < COOLDOWN_MS) {
      return NextResponse.json(
        {
          error:
            lang === "ko"
              ? "ë„ˆë¬´ ë¹ ë¦…ë‹ˆë‹¤. 3ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
              : "Too fast. Please wait 3 seconds.",
        },
        { status: 429 }
      );
    }
    lastRequestTime.set(ip, now);

    // Build context and prompt
    const portfolioContext = buildPortfolioContext(lang);
    const systemPrompt = getSystemPrompt(lang);

    // Detect response type and apply template
    const messageL = message.toLowerCase();
    let templateInstructions = "";

    if (messageL.includes("ìš”ì•½") || messageL.includes("summary")) {
      templateInstructions = lang === "ko"
        ? "\n## ì‘ë‹µ í˜•ì‹\n- 3-4ê°œ í•µì‹¬ í¬ì¸íŠ¸ë§Œ ë¶ˆë¦¿ìœ¼ë¡œ\n- ì˜ì‚¬ê²°ì • ë””í…Œì¼ì€ ì œì™¸\n- ê°„ê²°í•˜ê²Œ 30ì´ˆ ë¶„ëŸ‰"
        : "\n## RESPONSE FORMAT\n- 3-4 key bullet points only\n- No decision details\n- Concise, 30-second read";
    } else if (messageL.includes("ì˜ì‚¬ê²°ì •") || messageL.includes("decision") || messageL.includes("trade-off")) {
      templateInstructions = lang === "ko"
        ? "\n## ì‘ë‹µ í˜•ì‹ (ê³ ì •)\n1. **ì„ íƒí•œ ì ‘ê·¼**: [ë¬´ì—‡ì„ ì„ íƒí–ˆëŠ”ì§€]\n2. **ê²€í† í•œ ëŒ€ì•ˆë“¤**: [ê³ ë ¤í–ˆë˜ ë‹¤ë¥¸ ë°©ë²•ë“¤]\n3. **ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘**: [ì£¼ìš” ë¦¬ìŠ¤í¬ì™€ ì™„í™” ë°©ë²•]\n4. **ê²°ì •ì´ ìœ íš¨í–ˆë˜ ì´ìœ **: [ì™œ ì´ ë°©ë²•ì´ ì‘ë™í–ˆëŠ”ì§€]"
        : "\n## RESPONSE FORMAT (FIXED)\n1. **Chosen Approach**: [What was selected]\n2. **Alternatives Considered**: [Other options evaluated]\n3. **Risks & Mitigations**: [Key risks and how addressed]\n4. **Why This Worked**: [Validation of the decision]";
    } else if (messageL.includes("ì‹œìŠ¤í…œ") || messageL.includes("ë‹¤ì´ì–´ê·¸ë¨") || messageL.includes("system") || messageL.includes("diagram") || messageL.includes("ì•„í‚¤í…ì²˜") || messageL.includes("architecture")) {
      templateInstructions = lang === "ko"
        ? "\n## ì‘ë‹µ í˜•ì‹ (ê³ ì •)\n1. **ì‹œìŠ¤í…œ ê°œìš”**: [ì´ ì‹œìŠ¤í…œì´ ë¬´ì—‡ì¸ì§€]\n2. **êµ¬ì„± ìš”ì†Œ ìƒí˜¸ì‘ìš©**: [ì»´í¬ë„ŒíŠ¸ë“¤ì´ ì–´ë–»ê²Œ ì—°ë™ë˜ëŠ”ì§€]\n3. **ìš´ì˜ ê³ ë ¤ì‚¬í•­**: [ì‹¤ì œ ìš´ì˜ ì‹œ ì¤‘ìš”í•œ ì ]"
        : "\n## RESPONSE FORMAT (FIXED)\n1. **System Overview**: [What this system is]\n2. **Component Interactions**: [How components work together]\n3. **Operational Considerations**: [What matters in production]";
    }

    const fullPrompt = `
${systemPrompt}

## INSTRUCTIONS
- Use ONLY the provided CONTEXT below.
- Answer in ${lang === "ko" ? "Korean" : "English"}.
- Be concise unless user asks for detail.
- Use bullet points when listing.
- DO NOT invent facts or numbers.
- DO NOT provide percentages, satisfaction scores, or quantitative claims unless explicitly in context.
${templateInstructions}

## CONTEXT
${portfolioContext}

## USER QUESTION
${message}

## YOUR ANSWER
`.trim();

    // Gemini API version fallback (based on Google AI Studio 2026-01)
    // Ordered from newest/fastest to stable fallbacks
    const modelVersions = [
      "gemini-2.5-flash",               // ğŸš€ 2.5 Flash (5 RPM free)
      "gemini-2.5-flash-lite",          // âš¡ 2.5 Flash Lite (10 RPM free)
      "gemini-3-flash",                 // ğŸ†• Gemini 3 Flash (5 RPM free)
      "gemini-2.0-flash",               // âœ… 2.0 Flash
      "gemini-2.0-flash-exp",           // ğŸ§ª 2.0 Flash Experimental
      "gemini-1.5-flash",               // ğŸ’ 1.5 Flash (most stable)
      "gemini-1.5-flash-latest",        // ğŸ“Œ 1.5 Flash Latest
      "gemini-1.5-flash-002",           // ğŸ”· 1.5 Flash v002
      "gemini-1.5-flash-001",           // ğŸ”· 1.5 Flash v001
      "gemini-1.5-pro",                 // ğŸ¯ 1.5 Pro
      "gemini-1.5-pro-latest",          // ğŸ¯ 1.5 Pro Latest
      "gemini-1.0-pro",                 // ğŸ“¦ 1.0 Pro
      "gemini-pro",                     // ğŸ“¦ Legacy Pro
      "models/gemini-2.5-flash",        // ğŸ”„ With prefix
      "models/gemini-1.5-flash",        // ğŸ”„ With prefix
      "models/gemini-pro",              // ğŸ”„ With prefix
    ];

    let lastError: Error | null = null;

    // Try each model version until one succeeds
    for (const modelName of modelVersions) {
      try {
        const model = genAI.getGenerativeModel({
          model: modelName,
          generationConfig: {
            temperature: 0.7,
            maxOutputTokens: 1000,
          },
        });

        const result = await model.generateContent(fullPrompt);
        const response = result.response;
        const text = response.text();

        // Success - return response
        return NextResponse.json({
          response: text,
          remaining: rateLimit.remaining,
          modelUsed: modelName, // Optional: for debugging
        });
      } catch (error: any) {
        // Log detailed error for debugging
        console.warn(`Model ${modelName} failed:`, error.message);
        console.warn(`Error details:`, error);
        lastError = error;
        continue;
      }
    }

    // All models failed
    console.error("All Gemini models failed. Last error:", lastError?.message);
    return NextResponse.json(
      { error: "Failed to generate response" },
      { status: 500 }
    );
  } catch (error: any) {
    // Generic error (don't leak details)
    console.error("AI API error:", error.message);
    return NextResponse.json(
      { error: "Failed to generate response" },
      { status: 500 }
    );
  }
}
